{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wav\n",
    "import pywt\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import butter, filtfilt, iirnotch, medfilt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02020ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path\n",
    "folder_path = \"/Users/gracegerwe/Documents/Neuralink Raw Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f849546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all filtered amplitudes before setting threshold\n",
    "all_filtered_data = []\n",
    "all_spike_counts = []\n",
    "all_isi = []\n",
    "all_spike_features = []\n",
    "filtered_data_dict = {}\n",
    "file_count = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e324b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Bandpass Filter\n",
    "def bandpass_filter(data, lowcut=30, highcut=3000, sr=19531, order=4):\n",
    "    nyquist = sr / 2\n",
    "    low, high = lowcut / nyquist, highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2011c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Notch Filter at 60Hz and harmonics\n",
    "def notch_filter(data, freq=60, sr=19531, quality_factor=30):\n",
    "    nyquist = sr / 2\n",
    "    w0 = freq / nyquist\n",
    "    b, a = iirnotch(w0, quality_factor)\n",
    "    return filtfilt(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735ce30",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Wavelet Denoising\n",
    "def wavelet_denoise(data, wavelet='db4', level=2):\n",
    "    coeffs = pywt.wavedec(data, wavelet, mode=\"per\")\n",
    "    coeffs[1:] = [pywt.threshold(c, np.std(c), mode='soft') for c in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, wavelet, mode=\"per\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776509e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Add this function after your existing filter functions\n",
    "def detect_spikes(data, threshold, sr):\n",
    "    \"\"\"\n",
    "    Detect spikes with characteristic AP features:\n",
    "    1. Rapid depolarization (steep negative slope)\n",
    "    2. Peak amplitude above threshold\n",
    "    3. Repolarization phase (positive slope after peak)\n",
    "    4. Appropriate spike width (0.2-2ms)\n",
    "    \"\"\"\n",
    "    min_spike_width = int(0.0002 * sr)  # 0.2ms minimum width\n",
    "    max_spike_width = int(0.002 * sr)    # 2ms maximum width\n",
    "    min_spike_distance = int(0.002 * sr)  # Enforce minimum 2ms between spikes\n",
    "    spikes = []\n",
    "    \n",
    "    # Find all threshold crossings\n",
    "    threshold_crossings = np.where(data < -threshold)[0]\n",
    "    \n",
    "    # Add minimum distance requirement\n",
    "    if len(threshold_crossings) > 0:\n",
    "        # Keep only peaks that are separated by at least min_spike_distance\n",
    "        keep_spike = np.insert(np.diff(threshold_crossings) >= min_spike_distance, 0, True)\n",
    "        threshold_crossings = threshold_crossings[keep_spike]\n",
    "    \n",
    "    for i in threshold_crossings:\n",
    "        # Check if we have enough data for the window\n",
    "        if i < min_spike_width or i > len(data) - max_spike_width:\n",
    "            continue\n",
    "            \n",
    "        # Extract potential spike window\n",
    "        window = data[i-min_spike_width:i+max_spike_width]\n",
    "        \n",
    "        # Ensure window has enough points\n",
    "        if len(window) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Check for characteristic features\n",
    "        min_idx = np.argmin(window)\n",
    "        \n",
    "        # Ensure we have enough points for slope calculation\n",
    "        if min_idx < 1 or min_idx >= len(window) - 1:\n",
    "            continue\n",
    "            \n",
    "        # Compute slopes with safety checks\n",
    "        pre_window = window[:min_idx]\n",
    "        post_window = window[min_idx:]\n",
    "        \n",
    "        if len(pre_window) > 1 and len(post_window) > 1:\n",
    "            pre_slope = np.mean(np.diff(pre_window))\n",
    "            post_slope = np.mean(np.diff(post_window))\n",
    "            \n",
    "            # Criteria for valid spike\n",
    "            is_valid_spike = (\n",
    "                pre_slope < -threshold/10 and          # Steep depolarization\n",
    "                post_slope > threshold/20 and          # Clear repolarization\n",
    "                min_idx > 2 and                        # Not at the very start\n",
    "                min_idx < len(window) - 2              # Not at the very end\n",
    "            )\n",
    "            \n",
    "            if is_valid_spike:\n",
    "                spikes.append(i)\n",
    "    \n",
    "    return np.array(spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Loop through all files to collect filtered data\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Load the WAV file\n",
    "        sr, data = wav.read(file_path)\n",
    "\n",
    "        # Remove DC bias\n",
    "        data_centered = data - np.mean(data)\n",
    "\n",
    "        # Bandpass Filter\n",
    "        filtered_data = bandpass_filter(data_centered, lowcut=300, highcut=3000, sr=sr)\n",
    "\n",
    "        # Notch Filter at 60Hz and harmonics\n",
    "        for notch_freq in [60, 120, 180]:\n",
    "            filtered_data = notch_filter(filtered_data, freq=notch_freq, sr=sr)\n",
    "\n",
    "        # Wavelet Denoising\n",
    "        filtered_data = wavelet_denoise(filtered_data)\n",
    "\n",
    "        # Store filtered data for later use\n",
    "        filtered_data_dict[filename] = filtered_data\n",
    "\n",
    "        # Apply Median Filter for Motion Artifacts\n",
    "        filtered_data = medfilt(filtered_data, kernel_size=3)\n",
    "\n",
    "        # Store filtered data\n",
    "        all_filtered_data.extend(filtered_data)\n",
    "\n",
    "        file_count += 1\n",
    "        if file_count % 50 == 0:\n",
    "            print(f\"Filtered {file_count} files so far...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final confirmation that all files have been filtered\n",
    "print(f\"I have filtered all {file_count} files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddef84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute a **global threshold** based on all files\n",
    "global_threshold = np.percentile(np.abs(all_filtered_data), 99)\n",
    "print(f\"Global threshold set at: {global_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count = 0  # Reset file counter for spike detection\n",
    "for filename in sorted(filtered_data_dict.keys()):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        # Retrieve precomputed filtered data\n",
    "        filtered_data = filtered_data_dict[filename]\n",
    "        \n",
    "        # Detect spikes using improved detection\n",
    "        spike_indices = detect_spikes(filtered_data, global_threshold, sr)\n",
    "        \n",
    "        # Apply refractory period (2ms)\n",
    "        min_spike_distance = int(0.002 * sr)  # Shortened from 10ms to 2ms\n",
    "        if len(spike_indices) > 0:\n",
    "            spike_times = spike_indices[np.insert(np.diff(spike_indices) > min_spike_distance, 0, True)]\n",
    "        else:\n",
    "            spike_times = np.array([])\n",
    "\n",
    "        # Compute ISI\n",
    "        isi = np.diff(spike_times) / sr\n",
    "        all_isi.extend(isi)\n",
    "        all_spike_counts.append(len(spike_times))\n",
    "\n",
    "        # Extract spike features\n",
    "        window_size = int(0.002 * sr)  # 2ms window (~40 samples at 20kHz)\n",
    "        half_window = window_size // 2\n",
    "\n",
    "        for spike in spike_times:\n",
    "            if spike - half_window > 0 and spike + half_window < len(filtered_data):\n",
    "                spike_waveform = filtered_data[spike - half_window: spike + half_window]\n",
    "                \n",
    "                # Find spike features with more robust criteria\n",
    "                min_amp = np.min(spike_waveform)\n",
    "                max_amp = np.max(spike_waveform)\n",
    "                mid_amp = spike_waveform[len(spike_waveform) // 2]\n",
    "                \n",
    "                # Calculate timing features relative to spike peak\n",
    "                peak_idx = np.argmin(spike_waveform)\n",
    "                min_time = (peak_idx - half_window) / sr * 1000\n",
    "                max_time = (np.argmax(spike_waveform[peak_idx:]) + peak_idx - half_window) / sr * 1000\n",
    "                mid_time = 0\n",
    "                \n",
    "                # More robust SNR calculation\n",
    "                noise_window = filtered_data[max(0, spike - window_size):spike]\n",
    "                noise_std = np.std(noise_window)\n",
    "                spike_amplitude = abs(min_amp - np.mean(noise_window))\n",
    "                snr = spike_amplitude / noise_std\n",
    "                spike_prob = 1 / (1 + np.exp(-snr))\n",
    "                \n",
    "                all_spike_features.append([spike, min_amp, mid_amp, max_amp, min_time, mid_time, max_time, spike_prob])\n",
    "\n",
    "        # Print progress every 50 files\n",
    "        file_count += 1\n",
    "        if file_count % 50 == 0:\n",
    "            print(f\"Processed spike detection for {file_count} files...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742856a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final confirmation that all files have been processed for spikes\n",
    "print(f\"Spike detection completed for all {file_count} files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0608e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize the first action potential in the first channel\n",
    "first_file = sorted(filtered_data_dict.keys())[0]  # First file\n",
    "filtered_data = filtered_data_dict[first_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure filtering did not flatten spikes too much\n",
    "filtered_data = filtered_data - np.mean(filtered_data)  # Centering the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ee9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use detect_spikes instead of find_peaks\n",
    "all_spike_indices = detect_spikes(filtered_data, global_threshold, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501549bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_spike_indices) < 5:\n",
    "    print(f\"Warning: Only detected {len(all_spike_indices)} spikes, plotting all available.\")\n",
    "num_spikes = min(5, len(all_spike_indices))  # Ensure we don't exceed available spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82830988",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = int(0.007 * sr)  # 7ms window (~140 samples at 20kHz)\n",
    "half_window = window_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ec283",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack spikes with vertical offsets for clear visualization\n",
    "offset = np.max(np.abs(filtered_data)) * 1.2  # Dynamically set offset for better separation\n",
    "for i in range(num_spikes):\n",
    "    spike_time = all_spike_indices[i]\n",
    "\n",
    "    if spike_time - half_window > 0 and spike_time + half_window < len(filtered_data):\n",
    "        spike_waveform = filtered_data[spike_time - half_window : spike_time + half_window]\n",
    "        time_axis = np.linspace(0, 7, len(spike_waveform))  # Time in ms\n",
    "\n",
    "        plt.plot(time_axis, spike_waveform + i * offset, linewidth=2, label=f\"Spike {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Voltage (µV) (Offset Applied)\")\n",
    "plt.title(\"First Spike Detected in Channel 1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this before plotting to verify different spike times\n",
    "print(\"Spike times (ms):\")\n",
    "for i, spike_idx in enumerate(all_spike_indices[:3]):\n",
    "    print(f\"Spike {i+1}: {spike_idx/sr * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spike feature list to DataFrame\n",
    "spike_df = pd.DataFrame(\n",
    "    all_spike_features,\n",
    "    columns=[\"Index\", \"Neg. Hump Amp\", \"Mid Hump Amp\", \"Pos. Hump Amp\",\n",
    "             \"Neg. Hump Time\", \"Mid Hump Time\", \"Pos. Hump Time\", \"Spike Probability\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d95bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aggregated results\n",
    "axs[0].plot(range(len(all_spike_counts)), all_spike_counts, marker='o', linestyle='-', color='b')\n",
    "axs[0].set_xlabel(\"File Index\")\n",
    "axs[0].set_ylabel(\"Spike Count\")\n",
    "axs[0].set_title(\"Spike Counts Across All Files\")\n",
    "axs[0].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddfd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ISI Distribution\n",
    "axs[1].hist(isi, bins=50, edgecolor='black')\n",
    "axs[1].set_xlabel(\"Inter-Spike Interval (s)\")\n",
    "axs[1].set_ylabel(\"Count\")\n",
    "axs[1].set_title(\"ISI Distribution (Neural Spike Timing)\")\n",
    "axs[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a20543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average spike probability\n",
    "average_spike_probability = spike_df[\"Spike Probability\"].mean()\n",
    "print(f\"Average Spike Probability: {average_spike_probability:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949bab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average SNR\n",
    "average_snr = spike_df.apply(lambda row: (row[\"Pos. Hump Amp\"] - row[\"Neg. Hump Amp\"]) / (2 * np.std(filtered_data)), axis=1).mean()\n",
    "print(f\"Average SNR: {average_snr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of 10 Random Channels Over 2ms\n",
    "plt.figure(figsize=(12, 6))\n",
    "random_channels = random.sample(range(len(filtered_data_dict.keys())), min(10, len(filtered_data_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 1000  # Further decrease spacing for closer visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fca358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate(random_channels):\n",
    "    data = list(filtered_data_dict.values())[filename]\n",
    "    time_axis = np.linspace(0, 0.05, len(data[:int(sr * 0.05)]))  # First 20ms\n",
    "    plt.plot(time_axis, data[:int(sr * 0.05)] + i * spacing, label=f\"Channel {i+1}\")  # Offset for visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67ab47",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Relative Amplitude (µV)\")\n",
    "plt.title(\"Neural Data Visualization (10 Random Channels)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b0734",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sort_spikes(spike_waveforms, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Sort spikes using feature extraction and clustering\n",
    "    Parameters:\n",
    "        spike_waveforms: List of spike waveforms\n",
    "        n_clusters: Number of different neuron types to identify\n",
    "    Returns:\n",
    "        labels: Cluster assignments for each spike\n",
    "        features_df: DataFrame containing spike features\n",
    "    \"\"\"\n",
    "    if len(spike_waveforms) < n_clusters:\n",
    "        return np.zeros(len(spike_waveforms)), None\n",
    "        \n",
    "    # Extract features from each spike\n",
    "    features = []\n",
    "    for waveform in spike_waveforms:\n",
    "        # Find the three key points (negative, middle, positive humps)\n",
    "        neg_idx = np.argmin(waveform)\n",
    "        neg_amp = waveform[neg_idx]\n",
    "        neg_time = neg_idx\n",
    "        \n",
    "        # Middle point (zero crossing after negative peak)\n",
    "        mid_idx = neg_idx + np.argmin(np.abs(waveform[neg_idx:]))\n",
    "        mid_amp = waveform[mid_idx] if mid_idx < len(waveform) else 0\n",
    "        mid_time = mid_idx\n",
    "        \n",
    "        # Positive peak after negative peak\n",
    "        pos_idx = neg_idx + np.argmax(waveform[neg_idx:])\n",
    "        pos_amp = waveform[pos_idx] if pos_idx < len(waveform) else 0\n",
    "        pos_time = pos_idx\n",
    "        \n",
    "        # Calculate spike probability based on SNR\n",
    "        noise_std = np.std(waveform[:10])  # Use first 10 samples as noise estimate\n",
    "        snr = abs(neg_amp) / noise_std if noise_std != 0 else 0\n",
    "        spike_prob = 1 / (1 + np.exp(-snr))  # Sigmoid function for probability\n",
    "        \n",
    "        features.append([neg_amp, mid_amp, pos_amp, \n",
    "                        neg_time, mid_time, pos_time, \n",
    "                        spike_prob])\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Cluster spikes\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    # Create DataFrame with all features\n",
    "    features_df = pd.DataFrame(\n",
    "        features,\n",
    "        columns=[\"Neg. Hump Amp\", \"Mid Hump Amp\", \"Pos. Hump Amp\",\n",
    "                 \"Neg. Hump Time\", \"Mid Hump Time\", \"Pos. Hump Time\",\n",
    "                 \"Spike Probability\"]\n",
    "    )\n",
    "    features_df[\"Neuron Class\"] = labels\n",
    "    \n",
    "    return labels, features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ae776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After spike detection, collect waveforms and sort them\n",
    "spike_waveforms = []\n",
    "for spike in spike_times:\n",
    "    if spike - half_window > 0 and spike + half_window < len(filtered_data):\n",
    "        waveform = filtered_data[spike - half_window : spike + half_window]\n",
    "        spike_waveforms.append(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort spikes if we have enough\n",
    "if len(spike_waveforms) > 0:\n",
    "    spike_labels, features_df = sort_spikes(spike_waveforms)\n",
    "    features_df[\"Neuron Class\"] = features_df[\"Neuron Class\"] + 1\n",
    "    \n",
    "    print(\"\\nNeuron Classification Criteria:\")\n",
    "    print(\"Class 1 - Fast-Spiking Interneurons:\")\n",
    "    print(\"- Narrow spike width (<0.5ms)\")\n",
    "    print(\"- Small negative amplitude\")\n",
    "    print(\"- Quick repolarization\")\n",
    "    print(\"- High firing rate potential\")\n",
    "    \n",
    "    print(\"\\nClass 2 - Regular-Spiking Pyramidal Neurons:\")\n",
    "    print(\"- Medium spike width (0.5-1.0ms)\")\n",
    "    print(\"- Medium-large negative amplitude\")\n",
    "    print(\"- Pronounced after-hyperpolarization\")\n",
    "    print(\"- Regular firing pattern\")\n",
    "    \n",
    "    print(\"\\nClass 3 - Burst-Spiking Neurons:\")\n",
    "    print(\"- Wide spike width (>1.0ms)\")\n",
    "    print(\"- Large amplitude\")\n",
    "    print(\"- Complex spike shape\")\n",
    "    print(\"- Tendency for burst firing\")\n",
    "    \n",
    "    print(\"\\nNeuron Classification Results:\")\n",
    "    class_counts = features_df[\"Neuron Class\"].value_counts().sort_index()\n",
    "    \n",
    "    for class_num in class_counts.index:\n",
    "        count = class_counts[class_num]\n",
    "        class_data = features_df[features_df[\"Neuron Class\"] == class_num]\n",
    "        \n",
    "        # Calculate detailed characteristics for this class\n",
    "        mean_neg_amp = class_data[\"Neg. Hump Amp\"].mean()\n",
    "        mean_pos_amp = class_data[\"Pos. Hump Amp\"].mean()\n",
    "        mean_width = (class_data[\"Pos. Hump Time\"] - class_data[\"Neg. Hump Time\"]).mean()\n",
    "        mean_prob = class_data[\"Spike Probability\"].mean()\n",
    "        \n",
    "        print(f\"\\nNeuron Class {class_num}:\")\n",
    "        print(f\"Number of spikes: {count}\")\n",
    "        print(f\"Average spike width: {mean_width:.2f} samples\")\n",
    "        print(f\"Average negative amplitude: {mean_neg_amp:.2f} µV\")\n",
    "        print(f\"Average positive amplitude: {mean_pos_amp:.2f} µV\")\n",
    "        print(f\"Average spike probability: {mean_prob:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
